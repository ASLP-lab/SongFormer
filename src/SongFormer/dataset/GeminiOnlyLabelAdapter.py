# 1. It was found that the annotations generated by Gemini are discontinuous between segments
# (possibly differing by more than 1.7 seconds, accounting for approximately 1/4 to 1/3 of the cases).
# 2. Gemini's labels can compete with our SOTA model, but Gemini's boundary metrics are very poor.
# With a tolerance of 3 seconds, they are similar to the metrics of our best model.
import pdb
import random
import os
from collections import defaultdict
from pathlib import Path
import json
from venv import logger
import numpy as np
import math
from .label2id import (
    DATASET_ID_ALLOWED_LABEL_IDS,
    DATASET_LABEL_TO_DATASET_ID,
    ID_TO_LABEL,
    LABEL_TO_ID,
)
from argparse import Namespace
from scipy.ndimage import gaussian_filter1d
from .DatasetAdaper import DatasetAdapter
from omegaconf import ListConfig
import copy


# Adapter for datasets labeled only by Gemini
class GeminiOnlyLabelAdapter(DatasetAdapter):
    def __init__(self, **kwargs):
        (
            label_paths,
            hparams,
            internal_tmp_id,
            dataset_type,
            input_embedding_dir,
            split_ids_path,
        ) = (
            kwargs["label_paths"],
            kwargs["hparams"],
            kwargs["internal_tmp_id"],
            kwargs["dataset_type"],
            kwargs["input_embedding_dir"],
            kwargs["split_ids_path"],
        )
        self.frame_rates = hparams.frame_rates
        self.hparams = hparams
        self.label_to_id = LABEL_TO_ID
        self.dataset_id_to_dataset_id = DATASET_LABEL_TO_DATASET_ID
        self.id_to_label = ID_TO_LABEL
        self.internal_tmp_id = internal_tmp_id
        self.dataset_type = dataset_type
        self.EPS = 1e-6
        self.dataset_id2label_mask = {}
        for key, allowed_ids in DATASET_ID_ALLOWED_LABEL_IDS.items():
            self.dataset_id2label_mask[key] = np.ones(
                self.hparams.num_classes, dtype=bool
            )
            self.dataset_id2label_mask[key][allowed_ids] = False

        self.id2segments = {}
        data = self.load_jsonl(label_paths)

        self.input_embedding_dir = input_embedding_dir
        all_input_embedding_dirs = input_embedding_dir.split()

        valid_data_ids = self.get_ids_from_dir(all_input_embedding_dirs[0])

        for x in all_input_embedding_dirs:
            valid_data_ids = valid_data_ids.intersection(self.get_ids_from_dir(x))
        split_ids = []
        with open(split_ids_path) as f:
            for line in f:
                if not line.strip():
                    continue
                split_ids.append(line.strip())
        split_ids = set(split_ids)

        valid_data_ids = [
            x for x in valid_data_ids if "_".join(x.split("_")[:-1]) in split_ids
        ]
        valid_data_ids = [
            (internal_tmp_id, dataset_type, x, "HookTheoryAdapter")
            for x in valid_data_ids
        ]
        self.valid_data_ids = valid_data_ids
        rng = random.Random(42)
        rng.shuffle(self.valid_data_ids)
        for item in data:
            self.id2segments[item["data_id"]] = item["msa_info"]

    def get_ids_from_dir(self, dir_path: str):
        ids = os.listdir(dir_path)
        ids = [Path(x).stem for x in ids if x.endswith(".npy")]
        return set(ids)

    def time2frame(self, this_time):
        return int(this_time * self.frame_rates)

    def load_jsonl(self, paths):
        data = []
        for path in paths:
            with open(path, "r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    obj = json.loads(line)
                    data.append(obj)
        return data

    def get_ids(self):
        return list(self.valid_data_ids)

    def widen_temporal_events(self, events, num_neighbors):
        def theoretical_gaussian_max(sigma):
            return 1 / (np.sqrt(2 * np.pi) * sigma)

        widen_events = events
        sigma = num_neighbors / 3.0
        smoothed = gaussian_filter1d(widen_events.astype(float), sigma=sigma)
        smoothed /= theoretical_gaussian_max(sigma)
        smoothed = np.clip(smoothed, 0, 1)

        return smoothed

    def get_item_json(self, utt, start_time, end_time):
        embd_list = []
        embd_dirs = self.input_embedding_dir.split()
        for embd_dir in embd_dirs:
            if not Path(embd_dir).exists():
                raise FileNotFoundError(
                    f"Embedding directory {embd_dir} does not exist"
                )
            tmp = np.load(Path(embd_dir) / f"{utt}.npy").squeeze(axis=0)
            embd_list.append(tmp)

        # Check that max and min lengths of all representations differ by at most 2
        if len(embd_list) > 1:
            embd_shapes = [x.shape for x in embd_list]
            max_shape = max(embd_shapes, key=lambda x: x[0])
            min_shape = min(embd_shapes, key=lambda x: x[0])
            if abs(max_shape[0] - min_shape[0]) > 2:
                raise ValueError(
                    f"Embedding shapes differ too much: {max_shape} vs {min_shape}"
                )

        for idx in range(len(embd_list)):
            embd_list[idx] = embd_list[idx][: min_shape[0], :]

        input_embedding = np.concatenate(embd_list, axis=-1)

        return_json = self._get_item_json_without_embedding(
            "_".join(utt.split("_")[:-1]), start_time, end_time
        )

        if return_json is None:
            logger.warning(
                f"Skip {utt} because no valid segments found in {start_time} to {end_time}."
            )
            return None
        else:
            return_json["input_embedding"] = input_embedding
            return return_json

    def get_local_times_labels(self, utt):
        assert utt in self.id2segments, f"utt {utt} not found in id2segments"
        time_datas = [x[0] for x in self.id2segments[utt]]
        time_datas = list(map(float, time_datas))
        label_datas = [
            -1 if x[1] == "end" else self.label_to_id[x[1]]
            for x in self.id2segments[utt]
        ]
        return np.array(time_datas), label_datas

    def _get_item_json_without_embedding(self, utt, start_time, end_time):
        SLICE_DUR = int(math.ceil(end_time - start_time))

        local_times, local_labels = self.get_local_times_labels(utt)

        local_times, local_labels = (
            copy.deepcopy(local_times),
            copy.deepcopy(local_labels),
        )

        assert np.all(local_times[:-1] < local_times[1:]), (
            f"time must be sorted, but {utt} is {local_times}"
        )

        local_times = local_times - start_time

        time_L = max(0.0, float(local_times.min()))
        time_R = min(float(SLICE_DUR), float(local_times.max()))
        # Note whether boundary labels are reachable
        keep_boundarys = (time_L + self.EPS < local_times) & (
            local_times < time_R - self.EPS
        )

        # If no valid boundaries, return None
        if keep_boundarys.sum() <= 0:
            return None

        mask = np.ones([int(SLICE_DUR * self.frame_rates)], dtype=bool)
        mask[self.time2frame(time_L) : self.time2frame(time_R)] = False

        true_boundary = np.zeros([int(SLICE_DUR * self.frame_rates)], dtype=float)
        for idx in np.flatnonzero(keep_boundarys):
            true_boundary[self.time2frame(local_times[idx])] = 1

        true_function = np.zeros(
            [int(SLICE_DUR * self.frame_rates), self.hparams.num_classes],
            dtype=float,
        )
        true_function_list = []
        msa_info = []
        last_pos = self.time2frame(time_L)
        for idx in np.flatnonzero(keep_boundarys):

            true_function[
                last_pos : self.time2frame(local_times[idx]),
                int(local_labels[idx - 1]),
            ] = 1
            true_function_list.append(
                [int(x) for x in local_labels[idx - 1]]
                if isinstance(local_labels[idx - 1], list)
                else int(local_labels[idx - 1])
            )
            msa_info.append(
                (
                    float(max(local_times[idx - 1], time_L)),
                    [str(self.id_to_label[int(x)]) for x in local_labels[idx - 1]]
                    if isinstance(local_labels[idx - 1], list)
                    else str(self.id_to_label[int(local_labels[idx - 1])]),
                )
            )
            last_pos = self.time2frame(local_times[idx])

        # Check last label correctness
        true_function[
            last_pos : self.time2frame(time_R),
            local_labels[int(np.flatnonzero(keep_boundarys)[-1])],
        ] = 1
        true_function_list.append(
            [int(x) for x in local_labels[int(np.flatnonzero(keep_boundarys)[-1])]]
            if isinstance(local_labels[int(np.flatnonzero(keep_boundarys)[-1])], list)
            else int(local_labels[int(np.flatnonzero(keep_boundarys)[-1])])
        )

        msa_info.append(
            (
                float(local_times[int(np.flatnonzero(keep_boundarys)[-1])]),
                [
                    str(self.id_to_label[int(x)])
                    for x in local_labels[int(np.flatnonzero(keep_boundarys)[-1])]
                ]
                if isinstance(
                    local_labels[int(np.flatnonzero(keep_boundarys)[-1])], list
                )
                else str(
                    self.id_to_label[
                        int(local_labels[int(np.flatnonzero(keep_boundarys)[-1])])
                    ]
                ),
            )
        )
        # Append final label at end; decide if it's necessary
        msa_info.append((float(time_R), "end"))

        # Add boundary_mask & function_mask
        frame_len = int(SLICE_DUR * self.frame_rates)
        # During loss computation, boundaries are fully masked
        boundary_mask = np.ones([frame_len], dtype=bool)
        function_mask = np.zeros([frame_len], dtype=bool)

        # Set masks according to msa_info
        for i in range(len(msa_info) - 1):
            seg_start, seg_label = msa_info[i]
            seg_end, _ = msa_info[i + 1]
            start_frame = self.time2frame(seg_start)
            end_frame = self.time2frame(seg_end)

            # Handle case where label may be string or list
            is_no_label = (
                seg_label == "NO_LABEL"
                if isinstance(seg_label, str)
                else "NO_LABEL" in seg_label
            )

            if is_no_label:
                # function_mask set True
                function_mask[start_frame:end_frame] = True

        # ------~~------------
        # During loss computation, boundaries are fully masked
        boundary_mask = np.ones([frame_len], dtype=bool)
        function_mask = np.zeros([frame_len], dtype=bool)

        # Set masks according to msa_info
        for i in range(len(msa_info) - 1):
            seg_start, seg_label = msa_info[i]
            seg_end, _ = msa_info[i + 1]
            start_frame = self.time2frame(seg_start)
            end_frame = self.time2frame(seg_end)

            # Handle case where label may be string or list
            is_no_label = (
                seg_label == "NO_LABEL"
                if isinstance(seg_label, str)
                else "NO_LABEL" in seg_label
            )

            if is_no_label:
                # function_mask set True
                function_mask[start_frame:end_frame] = True

        # return all things except for input_embedding
        return {
            "data_id": self.internal_tmp_id + "_" + f"{utt}_{start_time}",
            "mask": mask,
            "true_boundary": true_boundary,
            "widen_true_boundary": self.widen_temporal_events(
                true_boundary, num_neighbors=self.hparams.num_neighbors
            ),
            "true_function": true_function,
            "true_function_list": true_function_list,
            "msa_info": msa_info,
            "dataset_id": self.dataset_id_to_dataset_id[self.dataset_type],
            "label_id_mask": self.dataset_id2label_mask[
                self.dataset_id_to_dataset_id[self.dataset_type]
            ],
            "boundary_mask": boundary_mask,  # Only effective during loss calculation
            "function_mask": function_mask,  # Only effective during loss calculation
        }